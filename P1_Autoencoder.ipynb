{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e31ec29-b009-4428-9ee6-726e197bdd46",
   "metadata": {},
   "source": [
    "### P1 Autoencoder NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15899281-cccd-4a44-87fa-5559d7dcd2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d852b378-c2b6-4601-9e3e-f63ab8063172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    X=np.eye(8)\n",
    "    y=X.copy()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "098484b6-3282-4d25-a95d-03a4b2a8ed6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderNetwork:\n",
    "    def __init__(self, learning_rate=1.0, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W1 = np.random.randn(8,3)*0.5\n",
    "        self.b1 = np.random.randn(1,3)*0.5\n",
    "        self.W2 = np.random.randn(3,8)*0.5\n",
    "        self.b2 = np.random.randn(1,8)*0.5\n",
    "        self.z1 = None\n",
    "        self.a1 = None\n",
    "        self.z2 = None\n",
    "        self.a2 = None\n",
    "\n",
    "        self.loss_history = []\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        z_val = np.clip(z, -500, 500)\n",
    "        return 1/(1+np.exp(-z_val))\n",
    "\n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s*(1-s)\n",
    "\n",
    "    def forward_pass(self, X):\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        self.a2 = self.sigmoid(self.z2)\n",
    "\n",
    "        return self.a2\n",
    "\n",
    "    def compute_loss(self, output, target):\n",
    "        m = output.shape[0]\n",
    "        loss = np.mean((output-target)**2)/2\n",
    "        return loss\n",
    "\n",
    "    def backward_pass(self, X, y):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        output_error = self.a2 - y\n",
    "        delta2 = output_error * self.sigmoid_derivative(self.z2)\n",
    "\n",
    "        hidden_error = delta2 @ self.W2.T\n",
    "        delta1 = hidden_error * self.sigmoid_derivative(self.z1)\n",
    "\n",
    "        dw2 = (self.a1.T @ delta2) / m\n",
    "        db2 = np.mean(delta2, axis=0, keepdims=True)\n",
    "\n",
    "        dw1 = (X.T @ delta1) / m\n",
    "        db1 = np.mean(delta1, axis=0, keepdims=True)\n",
    "\n",
    "        return dw1, db1, dw2, db2\n",
    "\n",
    "    def update_weights(self, dw1, db1, dw2, db2):\n",
    "        self.W1 -= self.learning_rate * dw1\n",
    "        self.b1 -= self.learning_rate * db1\n",
    "        self.W2 -= self.learning_rate * dw2\n",
    "        self.b2 -= self.learning_rate * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d02c845-8290-4bb4-99fa-27c1524ecfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, X, y, epochs=5000):\n",
    "    for epoch in range(epochs):\n",
    "        output = network.forward_pass(X)\n",
    "\n",
    "        loss = network.compute_loss(output, y)\n",
    "        network.loss_history.append(loss)\n",
    "\n",
    "        dw1, db1, dw2, db2 = network.backward_pass(X, y)\n",
    "\n",
    "        network.update_weights(dw1, db1, dw2, db2)\n",
    "\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch:4d}: Loss = {loss:.6f}\")\n",
    "\n",
    "    print(f\"\\n Training complete, final loss: {loss:.6f}\")\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cd322f92-4c41-4852-961b-25e7e0a08bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(network, X, y):\n",
    "    output = network.forward_pass(X)\n",
    "    hidden = network.a1\n",
    "\n",
    "    print(\"\\nReconstruction Results:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(\"Input Pattern -> Hidden Values -> Output Pattern\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "        input_str = ''.join([str(int(x)) for x in X[i]])\n",
    "        hidden_str = '[' + ', '.join([f\"{h:.3f}\" for h in hidden[i]]) + ']'\n",
    "        output_str = ''.join([str(int(round(x))) for x in output[i]])\n",
    "        print(f\"{input_str} -> {hidden_str} -> {output_str}\")\n",
    "        \n",
    "    # Calculate reconstruction accuracy\n",
    "    predictions_rounded = np.round(output)\n",
    "    accuracy = np.mean(predictions_rounded == y) * 100\n",
    "    print(f\"\\nReconstruction Accuracy: {accuracy:.1f}%\")\n",
    "    \n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f728b68-2617-4141-a0a1-2a971303391d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: Loss = 0.126346\n",
      "Epoch 1000: Loss = 0.024064\n",
      "Epoch 2000: Loss = 0.010835\n",
      "Epoch 3000: Loss = 0.008450\n",
      "Epoch 4000: Loss = 0.006754\n",
      "\n",
      " Training complete, final loss: 0.003289\n",
      "\n",
      "Reconstruction Results:\n",
      "------------------------------------------------------------\n",
      "Input Pattern -> Hidden Values -> Output Pattern\n",
      "------------------------------------------------------------\n",
      "10000000 -> [0.589, 0.028, 0.990] -> 10000000\n",
      "01000000 -> [0.927, 0.975, 0.026] -> 01000000\n",
      "00100000 -> [0.979, 0.802, 0.885] -> 00100000\n",
      "00010000 -> [0.080, 0.025, 0.034] -> 00010000\n",
      "00001000 -> [0.952, 0.026, 0.095] -> 00001000\n",
      "00000100 -> [0.212, 0.986, 0.987] -> 00000100\n",
      "00000010 -> [0.006, 0.217, 0.864] -> 00000010\n",
      "00000001 -> [0.017, 0.938, 0.081] -> 00000001\n",
      "\n",
      "Reconstruction Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    X, y = create_training_data()\n",
    "\n",
    "    network = AutoencoderNetwork(learning_rate=1.0)\n",
    "\n",
    "    network = train_network(network, X, y, epochs=5000)\n",
    "    \n",
    "    hidden_representations = analyze_results(network, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6e6d0-818d-4deb-b20b-b81ecd07c9b6",
   "metadata": {},
   "source": [
    "### B) Purpose of training algorithm in Autoencoder:\n",
    "\n",
    "The training algorithm teaches the network on how to compress information and then decompress it correctly. the autoencoder is like a bottleneck where, encoder (input->hidden) : here the network must squeeze 8 values down into 3 values then decoder (hidden -> output): must then expand those 3 values into the original 8 values. this helps the network learn which features are truly important, with only 3 neurons in the middle, encoder cannot store all information directly so it must find a encoding scheme. During decoder phase that is backpropagation, network discovers that each pattern can be represented as a unique combination of three hidden neurons. So the 3 hidden neurons contain a compressed representation that captures essential information and this helps to accurately rebuild the input. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
