{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72eca30c-a241-4edd-9e1d-c25c055c6d4e",
   "metadata": {},
   "source": [
    "### P5: CE Loss Manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bbb4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca151f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data():\n",
    "    wine = load_wine()\n",
    "    X = wine.data \n",
    "    y = wine.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def one_hot_encode(y, num_classes=3):\n",
    "    n = len(y)\n",
    "    y_one_hot = np.zeros((n, num_classes))\n",
    "    y_one_hot[np.arange(n), y] = 1\n",
    "    return y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99d805b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax activation function (numerically stable)\"\"\"\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b1a2bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    n = y_true.shape[0]\n",
    "    log_pred = np.log(y_pred + 1e-10)\n",
    "    loss = -np.sum(y_true * log_pred) / n\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d83c7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBLEM 5: MAX LIKELIHOOD NEURAL NETWORK CLASSIFIER\n",
      "======================================================================\n",
      "Epoch 200/1000 | Train Loss: 0.0241 | Train Acc: 100.00% | Test Loss: 0.0506 | Test Acc: 100.00%\n",
      "Epoch 400/1000 | Train Loss: 0.0089 | Train Acc: 100.00% | Test Loss: 0.0386 | Test Acc: 100.00%\n",
      "Epoch 600/1000 | Train Loss: 0.0052 | Train Acc: 100.00% | Test Loss: 0.0341 | Test Acc: 100.00%\n",
      "Epoch 800/1000 | Train Loss: 0.0036 | Train Acc: 100.00% | Test Loss: 0.0318 | Test Acc: 100.00%\n",
      "Epoch 1000/1000 | Train Loss: 0.0027 | Train Acc: 100.00% | Test Loss: 0.0304 | Test Acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "class MaxLikelihoodNeuralNetwork:\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 hidden_activation='relu', random_seed=42):\n",
    "        \"\"\"\n",
    "        Initialize neural network\n",
    "        \n",
    "        Parameters:\n",
    "        - input_size: number of input features\n",
    "        - hidden_size: number of neurons in hidden layer\n",
    "        - output_size: number of output classes (3 for wine)\n",
    "        - hidden_activation: 'relu' or 'sigmoid'\n",
    "        \"\"\"\n",
    "        np.random.seed(random_seed)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_activation = hidden_activation\n",
    "        \n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        \n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        \n",
    "        self.cache = {}\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward propagation\n",
    "        \n",
    "        X -> [W1, b1] -> z1 -> ReLU/Sigmoid -> a1 -> [W2, b2] -> z2 -> Softmax -> output\n",
    "        \"\"\"\n",
    "        n = X.shape[0]\n",
    "        \n",
    "        z1 = X @ self.W1 + self.b1\n",
    "        \n",
    "        if self.hidden_activation == 'relu':\n",
    "            a1 = relu(z1)  \n",
    "        else:\n",
    "            a1 = sigmoid(z1)  \n",
    "        \n",
    "        z2 = a1 @ self.W2 + self.b2 \n",
    "        output = softmax(z2)  \n",
    "        self.cache = {\n",
    "            'X': X,\n",
    "            'z1': z1,\n",
    "            'a1': a1,\n",
    "            'z2': z2,\n",
    "            'output': output,\n",
    "            'n': n\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        \"\"\"Backward propagation\n",
    "        \n",
    "        Uses the simplified derivative: d_loss/d_z2 = (output - y_true) / n\n",
    "        \"\"\"\n",
    "        X = self.cache['X']\n",
    "        z1 = self.cache['z1']\n",
    "        a1 = self.cache['a1']\n",
    "        output = self.cache['output']\n",
    "        n = self.cache['n']\n",
    "\n",
    "        dz2 = (output - y_true) / n\n",
    "        \n",
    "        dW2 = a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis=0, keepdims=True)\n",
    "        \n",
    "        da1 = dz2 @ self.W2.T\n",
    "        \n",
    "        if self.hidden_activation == 'relu':\n",
    "            dz1 = da1 * relu_derivative(z1)\n",
    "        else:\n",
    "            dz1 = da1 * sigmoid_derivative(z1)\n",
    "        \n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "    \n",
    "    def update_parameters(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "    \n",
    "    def train(self, X_train, y_train, X_test, y_test, \n",
    "              epochs=1000, learning_rate=0.1, print_every=100):\n",
    "        \n",
    "        y_train_oh = one_hot_encode(y_train, self.output_size)\n",
    "        y_test_oh = one_hot_encode(y_test, self.output_size)\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        train_accuracies = []\n",
    "        test_accuracies = []\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            output = self.forward(X_train)\n",
    "            \n",
    "            train_loss = cross_entropy_loss(y_train_oh, output)\n",
    "            \n",
    "            dW1, db1, dW2, db2 = self.backward(y_train_oh)\n",
    "            \n",
    "            self.update_parameters(dW1, db1, dW2, db2, learning_rate)\n",
    "            \n",
    "            test_output = self.forward(X_test)\n",
    "            test_loss = cross_entropy_loss(y_test_oh, test_output)\n",
    "            \n",
    "            train_acc = self.accuracy(X_train, y_train)\n",
    "            test_acc = self.accuracy(X_test, y_test)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            train_accuracies.append(train_acc)\n",
    "            test_accuracies.append(test_acc)\n",
    "\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} | \"\n",
    "                      f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "                      f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
    "        \n",
    "        return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "    \n",
    "    def predict(self, X):\n",
    "        output = self.forward(X)\n",
    "        predictions = np.argmax(output, axis=1)\n",
    "        return predictions\n",
    "    \n",
    "    def accuracy(self, X, y_true):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.mean(predictions == y_true) * 100\n",
    "        return accuracy\n",
    "        \n",
    "def main():\n",
    "    print(\"=\" * 70)\n",
    "    print(\"PROBLEM 5: MAX LIKELIHOOD NEURAL NETWORK CLASSIFIER\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = load_and_preprocess_data()\n",
    "    \n",
    "    input_size = X_train.shape[1] \n",
    "    hidden_size = 10  \n",
    "    output_size = 3  \n",
    "    \n",
    "    model = MaxLikelihoodNeuralNetwork(input_size=input_size, hidden_size=hidden_size, output_size=output_size, hidden_activation='relu')\n",
    "    \n",
    "    train_losses, test_losses, train_accs, test_accs = model.train(X_train, y_train, X_test, y_test, epochs=1000, learning_rate=0.1, print_every=200)    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa3fb5a-88b9-42f4-ae55-0a6913159c0c",
   "metadata": {},
   "source": [
    "Cross entropy loss is better than square loss for classification as it treats the problem as \"what's the prob of each class?\" instead of just calculating numerical differences. With softmax, the network calculates probabilities that add up to 1. Cross entropy the measures how wrong these probs are compared to the \"true\" answer. this gives network more strong learning values when it is taking a wrong step which helps it learn faster. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
