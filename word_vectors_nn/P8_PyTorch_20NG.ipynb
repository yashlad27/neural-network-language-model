{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8: Word Vectors - PyTorch NN (20 Newsgroups)\n",
    "\n",
    "**Dataset:** 20 Newsgroups (5-8 categories)  \n",
    "**Method:** Neural network trained on PPMI matrix (PyTorch + SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:58:58.732687Z",
     "start_time": "2025-11-11T17:58:57.585263Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:58:58.738995Z",
     "start_time": "2025-11-11T17:58:58.736788Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ]+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.split()\n",
    "\n",
    "def build_vocabulary(words, vocab_size=10000, min_count=3):\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',\n",
    "                  'is', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'this', 'that', 'it', 'as',\n",
    "                  'will', 'would', 'could', 'can', 'do', 'does', 'did', 'not', 'no', 'if', 'so'}\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    filtered = [(w, c) for w, c in word_counts.items() \n",
    "                if c >= min_count and len(w) >= 3 and w not in stop_words]\n",
    "    \n",
    "    most_common = sorted(filtered, key=lambda x: x[1], reverse=True)[:vocab_size]\n",
    "    word_to_id = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    corpus = [word_to_id[word] for word in words if word in word_to_id]\n",
    "    \n",
    "    return word_to_id, id_to_word, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:58:58.742713Z",
     "start_time": "2025-11-11T17:58:58.740923Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, vocab_size, window_size=5):\n",
    "    cooccur = torch.zeros((vocab_size, vocab_size), dtype=torch.float32)\n",
    "    \n",
    "    for i in tqdm(range(len(corpus)), desc=\"Building co-occurrence\"):\n",
    "        center = corpus[i]\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(corpus), i + window_size + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                context = corpus[j]\n",
    "                distance = abs(i - j)\n",
    "                weight = 1.0 / distance\n",
    "                cooccur[center, context] += weight\n",
    "    \n",
    "    return cooccur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPMI Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:58:58.746287Z",
     "start_time": "2025-11-11T17:58:58.744604Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ppmi(cooccur_matrix):\n",
    "    total = cooccur_matrix.sum()\n",
    "    word_counts = cooccur_matrix.sum(dim=1)\n",
    "    context_counts = cooccur_matrix.sum(dim=0)\n",
    "    \n",
    "    ppmi = torch.zeros_like(cooccur_matrix)\n",
    "    \n",
    "    for i in tqdm(range(cooccur_matrix.shape[0]), desc=\"Computing PPMI\"):\n",
    "        for j in range(cooccur_matrix.shape[1]):\n",
    "            if cooccur_matrix[i, j] > 0:\n",
    "                p_ij = cooccur_matrix[i, j] / total\n",
    "                p_i = word_counts[i] / total\n",
    "                p_j = context_counts[j] / total\n",
    "                \n",
    "                pmi = torch.log(p_ij / (p_i * p_j + 1e-10))\n",
    "                ppmi[i, j] = torch.clamp(pmi, min=0)\n",
    "    \n",
    "    ppmi.fill_diagonal_(0)\n",
    "    row_sums = ppmi.sum(dim=1, keepdim=True)\n",
    "    ppmi_normalized = ppmi / (row_sums + 1e-10)\n",
    "    \n",
    "    return ppmi_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:58:58.749684Z",
     "start_time": "2025-11-11T17:58:58.748014Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordVectorNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.W1 = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "        self.W2 = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        \n",
    "        nn.init.normal_(self.W1.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.W2.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, one_hot):\n",
    "        hidden = self.W1(one_hot)\n",
    "        output = self.W2(hidden)\n",
    "        probs = torch.softmax(output, dim=-1)\n",
    "        return probs\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        return self.W1.weight.t().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:58:58.754885Z",
     "start_time": "2025-11-11T17:58:58.752538Z"
    }
   },
   "outputs": [],
   "source": [
    "def manual_ce_loss(probs, target):\n",
    "    return -torch.sum(target * torch.log(probs + 1e-10))\n",
    "\n",
    "def train_model(model, ppmi_matrix, optimizer, epochs=10, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    ppmi_matrix = ppmi_matrix.to(device)\n",
    "    vocab_size = ppmi_matrix.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        indices = torch.randperm(vocab_size)\n",
    "        \n",
    "        for idx in tqdm(indices, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            one_hot = torch.zeros(vocab_size, device=device)\n",
    "            one_hot[idx] = 1\n",
    "            target = ppmi_matrix[idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            probs = model(one_hot)\n",
    "            loss = manual_ce_loss(probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / vocab_size\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:58:58.759053Z",
     "start_time": "2025-11-11T17:58:58.756968Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(v1, v2) / (norm1 * norm2)\n",
    "\n",
    "def find_similar_words(word, word_to_id, id_to_word, embeddings, top_k=15):\n",
    "    if word not in word_to_id:\n",
    "        return None\n",
    "    \n",
    "    word_idx = word_to_id[word]\n",
    "    word_emb = embeddings[word_idx]\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        if idx != word_idx:\n",
    "            other_emb = embeddings[idx]\n",
    "            sim = cosine_similarity(word_emb, other_emb)\n",
    "            similarities.append((id_to_word[idx], sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "def evaluate_model(embeddings, test_words, word_to_id, id_to_word):\n",
    "    for word in test_words:\n",
    "        similar = find_similar_words(word, word_to_id, id_to_word, embeddings)\n",
    "        if similar:\n",
    "            print(f\"\\n{word.upper()}:\")\n",
    "            for w, sim in similar:\n",
    "                print(f\"  {w:20s} {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:07:07.128379Z",
     "start_time": "2025-11-11T17:58:58.762178Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "Loading 20 Newsgroups...\n",
      "Total words: 1,133,058\n",
      "\n",
      "Building vocabulary...\n",
      "Vocab: 10,000, Corpus: 620,554\n",
      "\n",
      "Building co-occurrence matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence: 100%|██████████| 620554/620554 [00:17<00:00, 34807.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing PPMI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PPMI: 100%|██████████| 10000/10000 [03:55<00:00, 42.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 10000/10000 [00:23<00:00, 428.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg Loss: 9.2207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 10000/10000 [00:23<00:00, 432.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Avg Loss: 9.1822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 10000/10000 [00:23<00:00, 427.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Avg Loss: 9.1401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 10000/10000 [00:23<00:00, 431.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Avg Loss: 9.0830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 10000/10000 [00:23<00:00, 427.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Avg Loss: 8.9800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 10000/10000 [00:23<00:00, 431.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Avg Loss: 8.7667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 10000/10000 [00:23<00:00, 426.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Avg Loss: 8.5174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 10000/10000 [00:23<00:00, 429.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 8.3426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 10000/10000 [00:23<00:00, 427.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Avg Loss: 8.1744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10000/10000 [00:23<00:00, 424.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Avg Loss: 7.9960\n",
      "\n",
      "Extracting embeddings...\n",
      "Embeddings shape: (10000, 200)\n",
      "\n",
      "============================================================\n",
      "EVALUATION\n",
      "============================================================\n",
      "\n",
      "CHINA:\n",
      "  gods                 0.3936\n",
      "  expelled             0.3866\n",
      "  occupation           0.3837\n",
      "  ancestors            0.3712\n",
      "  couldn               0.3674\n",
      "  azerbaijanis         0.3583\n",
      "  unjust               0.3506\n",
      "  moscow               0.3488\n",
      "  awful                0.3487\n",
      "  soviet               0.3464\n",
      "  fought               0.3463\n",
      "  lives                0.3460\n",
      "  conscious            0.3427\n",
      "  families             0.3415\n",
      "  election             0.3385\n",
      "\n",
      "COMPUTER:\n",
      "  com                  0.4176\n",
      "  archive              0.4095\n",
      "  internet             0.4057\n",
      "  zip                  0.4039\n",
      "  nasa                 0.3978\n",
      "  atari                0.3959\n",
      "  binary               0.3924\n",
      "  calculus             0.3859\n",
      "  lynn                 0.3783\n",
      "  files                0.3738\n",
      "  boulder              0.3712\n",
      "  jumbo                0.3688\n",
      "  clone                0.3681\n",
      "  runs                 0.3660\n",
      "  cray                 0.3657\n",
      "\n",
      "PHONE:\n",
      "  fax                  0.4508\n",
      "  pcs                  0.3929\n",
      "  electronic           0.3823\n",
      "  pgp                  0.3767\n",
      "  enhanced             0.3758\n",
      "  server               0.3729\n",
      "  indiana              0.3697\n",
      "  fips                 0.3688\n",
      "  technologies         0.3677\n",
      "  internet             0.3645\n",
      "  ave                  0.3591\n",
      "  ontario              0.3514\n",
      "  summary              0.3491\n",
      "  stanford             0.3482\n",
      "  query                0.3481\n",
      "\n",
      "NAPOLEON:\n",
      "  welcome              0.3776\n",
      "  bigger               0.3638\n",
      "  favour               0.3560\n",
      "  responds             0.3555\n",
      "  marked               0.3548\n",
      "  weeks                0.3540\n",
      "  myself               0.3511\n",
      "  boys                 0.3499\n",
      "  distinguish          0.3497\n",
      "  banking              0.3465\n",
      "  cons                 0.3461\n",
      "  ira                  0.3452\n",
      "  somebody             0.3383\n",
      "  worlds               0.3380\n",
      "  fun                  0.3354\n",
      "\n",
      "GOD:\n",
      "  fellowship           0.3273\n",
      "  supernatural         0.3235\n",
      "  conception           0.3139\n",
      "  monophysitism        0.3108\n",
      "  incarnate            0.3036\n",
      "  mary                 0.2914\n",
      "  priest               0.2896\n",
      "  immaculate           0.2781\n",
      "  revealed             0.2768\n",
      "  spirit               0.2750\n",
      "  grace                0.2675\n",
      "  doctrines            0.2658\n",
      "  angel                0.2632\n",
      "  christ               0.2622\n",
      "  conscious            0.2597\n",
      "\n",
      "CATHOLIC:\n",
      "  witness              0.4329\n",
      "  hearing              0.3758\n",
      "  apostles             0.3741\n",
      "  lutheran             0.3615\n",
      "  historically         0.3543\n",
      "  deaf                 0.3542\n",
      "  bondage              0.3484\n",
      "  augustine            0.3395\n",
      "  incidentally         0.3384\n",
      "  commentary           0.3378\n",
      "  cor                  0.3372\n",
      "  pastor               0.3363\n",
      "  ages                 0.3350\n",
      "  mature               0.3348\n",
      "  explicitly           0.3335\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"\\nLoading 20 Newsgroups...\")\n",
    "categories = [\n",
    "    'comp.graphics', 'comp.sys.ibm.pc.hardware',\n",
    "    'sci.crypt', 'talk.politics.misc',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns',\n",
    "    'alt.atheism', 'talk.politics.mideast', 'talk.religion.misc', \n",
    "]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "text = ' '.join(newsgroups.data)\n",
    "words = preprocess_text(text)\n",
    "print(f\"Total words: {len(words):,}\")\n",
    "\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "word_to_id, id_to_word, corpus = build_vocabulary(words, vocab_size=10000)\n",
    "print(f\"Vocab: {len(word_to_id):,}, Corpus: {len(corpus):,}\")\n",
    "\n",
    "print(\"\\nBuilding co-occurrence matrix...\")\n",
    "cooccur = build_cooccurrence_matrix(corpus, len(word_to_id), window_size=5)\n",
    "\n",
    "print(\"\\nComputing PPMI...\")\n",
    "ppmi = compute_ppmi(cooccur)\n",
    "\n",
    "print(\"\\nInitializing model...\")\n",
    "model = WordVectorNN(vocab_size=len(word_to_id), embedding_dim=200)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.789)\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "train_model(model, ppmi, optimizer, epochs=10, device=device)\n",
    "\n",
    "print(\"\\nExtracting embeddings...\")\n",
    "embeddings = model.get_embeddings()\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "test_words = [\"china\", \"computer\", \"phone\", \"napoleon\", \"god\", \"catholic\"]\n",
    "evaluate_model(embeddings, test_words, word_to_id, id_to_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
