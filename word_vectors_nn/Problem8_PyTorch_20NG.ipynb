{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8: Word Vectors - PyTorch NN (20 Newsgroups)\n",
    "\n",
    "**Dataset:** 20 Newsgroups (5-8 categories)  \n",
    "**Method:** Neural network trained on PPMI matrix (PyTorch + SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ]+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.split()\n",
    "\n",
    "def build_vocabulary(words, vocab_size=5000, min_count=3):\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',\n",
    "                  'is', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'this', 'that', 'it', 'as',\n",
    "                  'will', 'would', 'could', 'can', 'do', 'does', 'did', 'not', 'no', 'if', 'so'}\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    filtered = [(w, c) for w, c in word_counts.items() \n",
    "                if c >= min_count and len(w) >= 3 and w not in stop_words]\n",
    "    \n",
    "    most_common = sorted(filtered, key=lambda x: x[1], reverse=True)[:vocab_size]\n",
    "    word_to_id = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    corpus = [word_to_id[word] for word in words if word in word_to_id]\n",
    "    \n",
    "    return word_to_id, id_to_word, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, vocab_size, window_size=5):\n",
    "    cooccur = torch.zeros((vocab_size, vocab_size), dtype=torch.float32)\n",
    "    \n",
    "    for i in tqdm(range(len(corpus)), desc=\"Building co-occurrence\"):\n",
    "        center = corpus[i]\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(corpus), i + window_size + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                context = corpus[j]\n",
    "                distance = abs(i - j)\n",
    "                weight = 1.0 / distance\n",
    "                cooccur[center, context] += weight\n",
    "    \n",
    "    return cooccur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPMI Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ppmi(cooccur_matrix):\n",
    "    total = cooccur_matrix.sum()\n",
    "    word_counts = cooccur_matrix.sum(dim=1)\n",
    "    context_counts = cooccur_matrix.sum(dim=0)\n",
    "    \n",
    "    ppmi = torch.zeros_like(cooccur_matrix)\n",
    "    \n",
    "    for i in tqdm(range(cooccur_matrix.shape[0]), desc=\"Computing PPMI\"):\n",
    "        for j in range(cooccur_matrix.shape[1]):\n",
    "            if cooccur_matrix[i, j] > 0:\n",
    "                p_ij = cooccur_matrix[i, j] / total\n",
    "                p_i = word_counts[i] / total\n",
    "                p_j = context_counts[j] / total\n",
    "                \n",
    "                pmi = torch.log(p_ij / (p_i * p_j + 1e-10))\n",
    "                ppmi[i, j] = torch.clamp(pmi, min=0)\n",
    "    \n",
    "    ppmi.fill_diagonal_(0)\n",
    "    row_sums = ppmi.sum(dim=1, keepdim=True)\n",
    "    ppmi_normalized = ppmi / (row_sums + 1e-10)\n",
    "    \n",
    "    return ppmi_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordVectorNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # No bias\n",
    "        self.W1 = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "        self.W2 = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        \n",
    "        # Initialize with different std\n",
    "        nn.init.normal_(self.W1.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.W2.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, one_hot):\n",
    "        hidden = self.W1(one_hot)\n",
    "        output = self.W2(hidden)\n",
    "        probs = torch.softmax(output, dim=-1)\n",
    "        return probs\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        return self.W1.weight.t().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_ce_loss(probs, target):\n",
    "    return -torch.sum(target * torch.log(probs + 1e-10))\n",
    "\n",
    "def train_model(ppmi_matrix, optimizer, epochs=10):\n",
    "    vocab_size = ppmi_matrix.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        indices = torch.randperm(vocab_size)\n",
    "        \n",
    "        for idx in tqdm(indices, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            one_hot = torch.zeros(vocab_size)\n",
    "            one_hot[idx] = 1\n",
    "            target = ppmi_matrix[idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            probs = model(one_hot)\n",
    "            loss = manual_ce_loss(probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / vocab_size\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(v1, v2) / (norm1 * norm2)\n",
    "\n",
    "def find_similar_words(word, word_to_id, id_to_word, embeddings, top_k=15):\n",
    "    if word not in word_to_id:\n",
    "        return None\n",
    "    \n",
    "    word_idx = word_to_id[word]\n",
    "    word_emb = embeddings[word_idx]\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        if idx != word_idx:\n",
    "            other_emb = embeddings[idx]\n",
    "            sim = cosine_similarity(word_emb, other_emb)\n",
    "            similarities.append((id_to_word[idx], sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "def evaluate_model(embeddings, test_words, word_to_id, id_to_word):\n",
    "    for word in test_words:\n",
    "        similar = find_similar_words(word, word_to_id, id_to_word, embeddings)\n",
    "        if similar:\n",
    "            print(f\"\\n{word.upper()}:\")\n",
    "            for w, sim in similar:\n",
    "                print(f\"  {w:20s} {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading 20 Newsgroups...\n",
      "Total words: 1,033,364\n",
      "\n",
      "Building vocabulary...\n",
      "Vocab: 5,000, Corpus: 522,453\n",
      "\n",
      "Building co-occurrence matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence: 100%|██████████| 522453/522453 [00:18<00:00, 27668.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing PPMI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PPMI: 100%|██████████| 5000/5000 [01:30<00:00, 55.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_model() got multiple values for argument 'epochs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.657\u001b[39m)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m train_model(model, ppmi, optimizer, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mExtracting embeddings...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     34\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_embeddings()\n",
      "\u001b[0;31mTypeError\u001b[0m: train_model() got multiple values for argument 'epochs'"
     ]
    }
   ],
   "source": [
    "print(\"\\nLoading 20 Newsgroups...\")\n",
    "categories = [\n",
    "    'comp.graphics', 'comp.sys.ibm.pc.hardware',\n",
    "    'sci.crypt', 'sci.space',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns', 'talk.politics.mideast',\n",
    "    'alt.atheism'\n",
    "]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories,\n",
    "                                remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "text = ' '.join(newsgroups.data)\n",
    "words = preprocess_text(text)\n",
    "print(f\"Total words: {len(words):,}\")\n",
    "\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "word_to_id, id_to_word, corpus = build_vocabulary(words, vocab_size=5000)\n",
    "print(f\"Vocab: {len(word_to_id):,}, Corpus: {len(corpus):,}\")\n",
    "\n",
    "print(\"\\nBuilding co-occurrence matrix...\")\n",
    "cooccur = build_cooccurrence_matrix(corpus, len(word_to_id), window_size=5)\n",
    "\n",
    "print(\"\\nComputing PPMI...\")\n",
    "ppmi = compute_ppmi(cooccur)\n",
    "\n",
    "print(\"\\nInitializing model...\")\n",
    "model = WordVectorNN(vocab_size=len(word_to_id), embedding_dim=200)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.657)\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "train_model(model, ppmi, optimizer, epochs=10)\n",
    "\n",
    "print(\"\\nExtracting embeddings...\")\n",
    "embeddings = model.get_embeddings()\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "test_words = [\"china\", \"computer\", \"phone\", \"napoleon\", \"god\", \"catholic\"]\n",
    "evaluate_model(embeddings, test_words, word_to_id, id_to_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m4-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
