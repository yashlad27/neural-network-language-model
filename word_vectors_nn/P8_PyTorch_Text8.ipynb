{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 8: Word Vectors - PyTorch NN (Text8)\n",
    "\n",
    "**Dataset:** Text8 (200K words)  \n",
    "**Method:** Neural network trained on PPMI matrix (PyTorch + SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:59:01.284161Z",
     "start_time": "2025-11-11T17:59:00.873751Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:59:01.289482Z",
     "start_time": "2025-11-11T17:59:01.287180Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ]+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.split()\n",
    "\n",
    "def build_vocabulary(words, vocab_size=5000, min_count=3):\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',\n",
    "                  'is', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'this', 'that', 'it', 'as'}\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    filtered = [(w, c) for w, c in word_counts.items() \n",
    "                if c >= min_count and len(w) >= 3 and w not in stop_words]\n",
    "    \n",
    "    most_common = sorted(filtered, key=lambda x: x[1], reverse=True)[:vocab_size]\n",
    "    word_to_id = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    corpus = [word_to_id[word] for word in words if word in word_to_id]\n",
    "    \n",
    "    return word_to_id, id_to_word, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:59:01.293537Z",
     "start_time": "2025-11-11T17:59:01.291777Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, vocab_size, window_size=5):\n",
    "    cooccur = torch.zeros((vocab_size, vocab_size), dtype=torch.float32)\n",
    "    \n",
    "    for i in tqdm(range(len(corpus)), desc=\"Building co-occurrence\"):\n",
    "        center = corpus[i]\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(corpus), i + window_size + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                context = corpus[j]\n",
    "                distance = abs(i - j)\n",
    "                weight = 1.0 / distance\n",
    "                cooccur[center, context] += weight\n",
    "    \n",
    "    return cooccur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPMI Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:59:01.297827Z",
     "start_time": "2025-11-11T17:59:01.295921Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ppmi(cooccur_matrix):\n",
    "    total = cooccur_matrix.sum()\n",
    "    word_counts = cooccur_matrix.sum(dim=1)\n",
    "    context_counts = cooccur_matrix.sum(dim=0)\n",
    "    \n",
    "    ppmi = torch.zeros_like(cooccur_matrix)\n",
    "    \n",
    "    for i in tqdm(range(cooccur_matrix.shape[0]), desc=\"Computing PPMI\"):\n",
    "        for j in range(cooccur_matrix.shape[1]):\n",
    "            if cooccur_matrix[i, j] > 0:\n",
    "                p_ij = cooccur_matrix[i, j] / total\n",
    "                p_i = word_counts[i] / total\n",
    "                p_j = context_counts[j] / total\n",
    "                \n",
    "                pmi = torch.log(p_ij / (p_i * p_j + 1e-10))\n",
    "                ppmi[i, j] = torch.clamp(pmi, min=0)\n",
    "    \n",
    "    ppmi.fill_diagonal_(0)\n",
    "    row_sums = ppmi.sum(dim=1, keepdim=True)\n",
    "    ppmi_normalized = ppmi / (row_sums + 1e-10)\n",
    "    \n",
    "    return ppmi_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:59:01.301491Z",
     "start_time": "2025-11-11T17:59:01.299653Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordVectorNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W1 = nn.Linear(vocab_size, embedding_dim, bias=False)\n",
    "        self.W2 = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        \n",
    "        nn.init.normal_(self.W1.weight, mean=0, std=0.1)\n",
    "        nn.init.normal_(self.W2.weight, mean=0, std=0.1)\n",
    "    \n",
    "    def forward(self, one_hot):\n",
    "        hidden = self.W1(one_hot)\n",
    "        output = self.W2(hidden)\n",
    "        probs = torch.softmax(output, dim=-1)\n",
    "        return probs\n",
    "    \n",
    "    def get_embeddings(self):\n",
    "        return self.W1.weight.t().detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:59:01.305480Z",
     "start_time": "2025-11-11T17:59:01.303543Z"
    }
   },
   "outputs": [],
   "source": [
    "def manual_ce_loss(probs, target):\n",
    "    return -torch.sum(target * torch.log(probs + 1e-10))\n",
    "\n",
    "def train_model(model, ppmi_matrix, optimizer, epochs=10, device='cpu'):\n",
    "    model = model.to(device)\n",
    "    ppmi_matrix = ppmi_matrix.to(device)\n",
    "    vocab_size = ppmi_matrix.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        indices = torch.randperm(vocab_size)\n",
    "        \n",
    "        for idx in tqdm(indices, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            one_hot = torch.zeros(vocab_size, device=device)\n",
    "            one_hot[idx] = 1\n",
    "            target = ppmi_matrix[idx]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            probs = model(one_hot)\n",
    "            loss = manual_ce_loss(probs, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / vocab_size\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:59:01.309148Z",
     "start_time": "2025-11-11T17:59:01.307019Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(v1, v2) / (norm1 * norm2)\n",
    "\n",
    "def find_similar_words(word, word_to_id, id_to_word, embeddings, top_k=15):\n",
    "    if word not in word_to_id:\n",
    "        return None\n",
    "    \n",
    "    word_idx = word_to_id[word]\n",
    "    word_emb = embeddings[word_idx]\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(len(embeddings)):\n",
    "        if idx != word_idx:\n",
    "            other_emb = embeddings[idx]\n",
    "            sim = cosine_similarity(word_emb, other_emb)\n",
    "            similarities.append((id_to_word[idx], sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "def evaluate_model(embeddings, test_words, word_to_id, id_to_word):\n",
    "    for word in test_words:\n",
    "        similar = find_similar_words(word, word_to_id, id_to_word, embeddings)\n",
    "        if similar:\n",
    "            print(f\"\\n{word.upper()}:\")\n",
    "            for w, sim in similar:\n",
    "                print(f\"  {w:20s} {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T18:00:57.075403Z",
     "start_time": "2025-11-11T17:59:01.311830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "\n",
      "Loading Text8...\n",
      "Total words: 199,999\n",
      "\n",
      "Building vocabulary...\n",
      "Vocab: 5,000, Corpus: 115,831\n",
      "\n",
      "Building co-occurrence matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence: 100%|██████████| 115831/115831 [00:03<00:00, 35132.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing PPMI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PPMI: 100%|██████████| 5000/5000 [00:59<00:00, 84.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 5000/5000 [00:05<00:00, 965.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg Loss: 8.5270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 5000/5000 [00:05<00:00, 968.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Avg Loss: 8.4681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 5000/5000 [00:05<00:00, 942.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Avg Loss: 8.4030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 5000/5000 [00:05<00:00, 956.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Avg Loss: 8.3180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 5000/5000 [00:05<00:00, 942.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Avg Loss: 8.1910\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 5000/5000 [00:05<00:00, 942.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Avg Loss: 7.9882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 5000/5000 [00:05<00:00, 954.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Avg Loss: 7.6973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 5000/5000 [00:05<00:00, 945.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 7.3849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 5000/5000 [00:05<00:00, 941.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Avg Loss: 7.0644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 5000/5000 [00:05<00:00, 959.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Avg Loss: 6.7417\n",
      "\n",
      "Extracting embeddings...\n",
      "Embeddings shape: (5000, 200)\n",
      "\n",
      "============================================================\n",
      "EVALUATION\n",
      "============================================================\n",
      "\n",
      "CHINA:\n",
      "  japan                0.6511\n",
      "  korea                0.6141\n",
      "  vietnam              0.6137\n",
      "  india                0.6046\n",
      "  myanmar              0.6027\n",
      "  singapore            0.6023\n",
      "  mongolia             0.5929\n",
      "  thailand             0.5914\n",
      "  cambodia             0.5649\n",
      "  malaysia             0.5616\n",
      "  laos                 0.5265\n",
      "  southeast            0.5148\n",
      "  buddhism             0.5146\n",
      "  siberia              0.5063\n",
      "  indonesia            0.4978\n",
      "\n",
      "COMPUTER:\n",
      "  ask                  0.3731\n",
      "  attitude             0.3393\n",
      "  representation       0.3201\n",
      "  why                  0.3155\n",
      "  animated             0.3088\n",
      "  true                 0.3078\n",
      "  internet             0.3023\n",
      "  comments             0.2973\n",
      "  variants             0.2955\n",
      "  windows              0.2938\n",
      "  design               0.2912\n",
      "  realized             0.2870\n",
      "  plutarch             0.2867\n",
      "  sequences            0.2867\n",
      "  enter                0.2811\n",
      "\n",
      "NAPOLEON:\n",
      "  snowball             0.5988\n",
      "  windmill             0.5270\n",
      "  stalin               0.4991\n",
      "  dogs                 0.4843\n",
      "  squealer             0.4671\n",
      "  farm                 0.4264\n",
      "  inspired             0.4036\n",
      "  becomes              0.3939\n",
      "  pig                  0.3879\n",
      "  frederick            0.3829\n",
      "  pigs                 0.3670\n",
      "  passionate           0.3603\n",
      "  battle               0.3536\n",
      "  boxer                0.3519\n",
      "  van                  0.3464\n",
      "\n",
      "GOD:\n",
      "  fire                 0.3463\n",
      "  mortal               0.3450\n",
      "  thy                  0.3279\n",
      "  rage                 0.3218\n",
      "  foundation           0.3198\n",
      "  ill                  0.3197\n",
      "  tears                0.3183\n",
      "  daughter             0.3135\n",
      "  lovers               0.3066\n",
      "  sin                  0.3061\n",
      "  fought               0.3032\n",
      "  philosophies         0.3019\n",
      "  crucial              0.3015\n",
      "  struggle             0.2995\n",
      "  leo                  0.2982\n",
      "\n",
      "CATHOLIC:\n",
      "  protestant           0.4719\n",
      "  religions            0.4658\n",
      "  muslim               0.3760\n",
      "  majority             0.3692\n",
      "  religion             0.3588\n",
      "  berbers              0.3496\n",
      "  morocco              0.3484\n",
      "  roman                0.3462\n",
      "  catholicism          0.3386\n",
      "  bishop               0.3361\n",
      "  india                0.3329\n",
      "  aruban               0.3213\n",
      "  predominant          0.3210\n",
      "  borders              0.3181\n",
      "  christian            0.3177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "print(\"\\nLoading Text8...\")\n",
    "with open('../text8_200K.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words = preprocess_text(text)\n",
    "print(f\"Total words: {len(words):,}\")\n",
    "\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "word_to_id, id_to_word, corpus = build_vocabulary(words, vocab_size=5000)\n",
    "print(f\"Vocab: {len(word_to_id):,}, Corpus: {len(corpus):,}\")\n",
    "\n",
    "print(\"\\nBuilding co-occurrence matrix...\")\n",
    "cooccur = build_cooccurrence_matrix(corpus, len(word_to_id), window_size=5)\n",
    "\n",
    "print(\"\\nComputing PPMI...\")\n",
    "ppmi = compute_ppmi(cooccur)\n",
    "\n",
    "print(\"\\nInitializing model...\")\n",
    "model = WordVectorNN(vocab_size=len(word_to_id), embedding_dim=200)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.789)\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "train_model(model, ppmi, optimizer, epochs=10, device=device)\n",
    "\n",
    "print(\"\\nExtracting embeddings...\")\n",
    "embeddings = model.get_embeddings()\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "test_words = [\"china\", \"computer\", \"phone\", \"napoleon\", \"god\", \"catholic\"]\n",
    "evaluate_model(embeddings, test_words, word_to_id, id_to_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
