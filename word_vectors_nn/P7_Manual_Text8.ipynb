{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7: Word Vectors - Manual NN (Text8)\n",
    "\n",
    "**Dataset:** Text8 (200K words)  \n",
    "**Method:** Neural network trained on PPMI matrix (Manual NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:54:02.926763Z",
     "start_time": "2025-11-11T17:54:02.890035Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:54:02.931628Z",
     "start_time": "2025-11-11T17:54:02.929424Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ]+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.split()\n",
    "\n",
    "def build_vocabulary(words, vocab_size=5000, min_count=3):\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',\n",
    "                  'is', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'this', 'that', 'it', 'as'}\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    filtered = [(w, c) for w, c in word_counts.items() \n",
    "                if c >= min_count and len(w) >= 3 and w not in stop_words]\n",
    "    \n",
    "    most_common = sorted(filtered, key=lambda x: x[1], reverse=True)[:vocab_size]\n",
    "    word_to_id = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    corpus = [word_to_id[word] for word in words if word in word_to_id]\n",
    "    \n",
    "    return word_to_id, id_to_word, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:54:02.935608Z",
     "start_time": "2025-11-11T17:54:02.934021Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, vocab_size, window_size=5):\n",
    "    cooccur = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for i in tqdm(range(len(corpus)), desc=\"Building co-occurrence\"):\n",
    "        center = corpus[i]\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(corpus), i + window_size + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                context = corpus[j]\n",
    "                distance = abs(i - j)\n",
    "                weight = 1.0 / distance\n",
    "                cooccur[center, context] += weight\n",
    "    \n",
    "    return cooccur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPMI Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:54:02.939090Z",
     "start_time": "2025-11-11T17:54:02.937290Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ppmi(cooccur_matrix):\n",
    "    total = cooccur_matrix.sum()\n",
    "    word_counts = cooccur_matrix.sum(axis=1)\n",
    "    context_counts = cooccur_matrix.sum(axis=0)\n",
    "    \n",
    "    ppmi = np.zeros_like(cooccur_matrix)\n",
    "    \n",
    "    for i in tqdm(range(cooccur_matrix.shape[0]), desc=\"Computing PPMI\"):\n",
    "        for j in range(cooccur_matrix.shape[1]):\n",
    "            if cooccur_matrix[i, j] > 0:\n",
    "                p_ij = cooccur_matrix[i, j] / total\n",
    "                p_i = word_counts[i] / total\n",
    "                p_j = context_counts[j] / total\n",
    "                \n",
    "                pmi = np.log(p_ij / (p_i * p_j + 1e-10))\n",
    "                ppmi[i, j] = max(0, pmi)\n",
    "    \n",
    "    np.fill_diagonal(ppmi, 0)\n",
    "    \n",
    "    row_sums = ppmi.sum(axis=1, keepdims=True)\n",
    "    ppmi_normalized = np.divide(ppmi, row_sums, where=row_sums>0)\n",
    "    \n",
    "    return ppmi_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:54:02.943056Z",
     "start_time": "2025-11-11T17:54:02.940831Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordVectorNN:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.W1 = np.random.normal(0, 0.1, (vocab_size, embedding_dim)).astype(np.float32)\n",
    "        self.W2 = np.random.normal(0, 0.1, (embedding_dim, vocab_size)).astype(np.float32)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / (np.sum(exp_x) + 1e-10)\n",
    "    \n",
    "    def forward(self, word_idx):\n",
    "        hidden = self.W1[word_idx] \n",
    "        output = hidden @ self.W2 \n",
    "        probs = self.softmax(output)\n",
    "        return hidden, probs\n",
    "    \n",
    "    def backward(self, word_idx, hidden, probs, target, lr):\n",
    "        d_output = probs - target \n",
    "        d_W2 = np.outer(hidden, d_output)  \n",
    "        d_hidden = d_output @ self.W2.T\n",
    "        d_W1 = d_hidden \n",
    "        self.W2 -= lr * d_W2\n",
    "        self.W1[word_idx] -= lr * d_W1\n",
    "    \n",
    "    def train_step(self, word_idx, target, lr):\n",
    "        hidden, probs = self.forward(word_idx)\n",
    "        loss = -np.sum(target * np.log(probs + 1e-10))\n",
    "        self.backward(word_idx, hidden, probs, target, lr)\n",
    "        return loss\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        return self.W1[word_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:54:02.946255Z",
     "start_time": "2025-11-11T17:54:02.944667Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, ppmi_matrix, epochs=10, lr=0.1):\n",
    "    vocab_size = ppmi_matrix.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        indices = np.arange(vocab_size)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for idx in tqdm(indices, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            loss = model.train_step(idx, ppmi_matrix[idx], lr)\n",
    "            total_loss += loss\n",
    "        \n",
    "        avg_loss = total_loss / vocab_size\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:54:02.949822Z",
     "start_time": "2025-11-11T17:54:02.947873Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(v1, v2) / (norm1 * norm2)\n",
    "\n",
    "def find_similar_words(word, word_to_id, id_to_word, model, top_k=15):\n",
    "    if word not in word_to_id:\n",
    "        return None\n",
    "    \n",
    "    word_idx = word_to_id[word]\n",
    "    word_emb = model.get_embedding(word_idx)\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(model.vocab_size):\n",
    "        if idx != word_idx:\n",
    "            other_emb = model.get_embedding(idx)\n",
    "            sim = cosine_similarity(word_emb, other_emb)\n",
    "            similarities.append((id_to_word[idx], sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "def evaluate_model(model, test_words, word_to_id, id_to_word):\n",
    "    for word in test_words:\n",
    "        similar = find_similar_words(word, word_to_id, id_to_word, model)\n",
    "        if similar:\n",
    "            print(f\"\\n{word.upper()}:\")\n",
    "            for w, sim in similar:\n",
    "                print(f\"  {w:20s} {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:55:04.025433Z",
     "start_time": "2025-11-11T17:54:02.952904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Text8...\n",
      "Total words: 199,999\n",
      "\n",
      "Building vocabulary...\n",
      "Vocab: 6,897, Corpus: 121,918\n",
      "\n",
      "Building co-occurrence matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence: 100%|██████████| 121918/121918 [00:00<00:00, 469306.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing PPMI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PPMI: 100%|██████████| 6897/6897 [00:03<00:00, 2040.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 6897/6897 [00:05<00:00, 1218.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg Loss: 8.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 6897/6897 [00:05<00:00, 1236.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Avg Loss: 8.7655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 6897/6897 [00:05<00:00, 1238.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Avg Loss: 8.6648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 6897/6897 [00:05<00:00, 1168.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Avg Loss: 8.5148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 6897/6897 [00:05<00:00, 1213.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Avg Loss: 8.2544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 6897/6897 [00:05<00:00, 1206.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Avg Loss: 7.8358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 6897/6897 [00:05<00:00, 1167.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Avg Loss: 7.3823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 6897/6897 [00:05<00:00, 1215.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 6.9226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 6897/6897 [00:05<00:00, 1187.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Avg Loss: 6.4757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 6897/6897 [00:05<00:00, 1225.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Avg Loss: 6.0846\n",
      "\n",
      "============================================================\n",
      "EVALUATION\n",
      "============================================================\n",
      "\n",
      "CHINA:\n",
      "  japan                0.6247\n",
      "  mongolia             0.5987\n",
      "  myanmar              0.5951\n",
      "  thailand             0.5836\n",
      "  sri                  0.5798\n",
      "  korea                0.5774\n",
      "  india                0.5716\n",
      "  asia                 0.5541\n",
      "  buddhism             0.5443\n",
      "  southeast            0.5393\n",
      "  singapore            0.5380\n",
      "  cambodia             0.5344\n",
      "  regions              0.5312\n",
      "  lanka                0.5285\n",
      "  malaysia             0.5279\n",
      "\n",
      "COMPUTER:\n",
      "  software             0.4113\n",
      "  equipment            0.3852\n",
      "  animated             0.3786\n",
      "  generated            0.3613\n",
      "  motion               0.3606\n",
      "  media                0.3524\n",
      "  probability          0.3522\n",
      "  ansi                 0.3436\n",
      "  control              0.3400\n",
      "  storage              0.3380\n",
      "  pages                0.3365\n",
      "  technique            0.3327\n",
      "  unusual              0.3309\n",
      "  navigation           0.3289\n",
      "  intended             0.3288\n",
      "\n",
      "PHONE:\n",
      "  gear                 0.4430\n",
      "  genesis              0.3804\n",
      "  tennis               0.3648\n",
      "  flag                 0.3376\n",
      "  mesa                 0.3367\n",
      "  planted              0.3299\n",
      "  bush                 0.3120\n",
      "  mobile               0.3023\n",
      "  took                 0.2911\n",
      "  call                 0.2883\n",
      "  nixon                0.2861\n",
      "  courts               0.2770\n",
      "  president            0.2762\n",
      "  contested            0.2751\n",
      "  astronauts           0.2668\n",
      "\n",
      "NAPOLEON:\n",
      "  snowball             0.5777\n",
      "  stalin               0.4931\n",
      "  squealer             0.4902\n",
      "  windmill             0.4664\n",
      "  farm                 0.4267\n",
      "  cowshed              0.4138\n",
      "  announces            0.3949\n",
      "  boxer                0.3874\n",
      "  pig                  0.3830\n",
      "  begins               0.3739\n",
      "  inspired             0.3712\n",
      "  frederick            0.3708\n",
      "  orwell               0.3667\n",
      "  trotsky              0.3651\n",
      "  manor                0.3638\n",
      "\n",
      "GOD:\n",
      "  prophecy             0.4474\n",
      "  sun                  0.3893\n",
      "  hermes               0.3536\n",
      "  loved                0.3527\n",
      "  flocks               0.3417\n",
      "  thy                  0.3403\n",
      "  thessaly             0.3348\n",
      "  defender             0.3336\n",
      "  equated              0.3327\n",
      "  love                 0.3318\n",
      "  morning              0.3316\n",
      "  greeks               0.3250\n",
      "  priam                0.3229\n",
      "  tears                0.3226\n",
      "  greek                0.3173\n",
      "\n",
      "CATHOLIC:\n",
      "  protestant           0.4337\n",
      "  roman                0.4135\n",
      "  church               0.4090\n",
      "  orthodox             0.3947\n",
      "  joan                 0.3733\n",
      "  religion             0.3732\n",
      "  france               0.3618\n",
      "  religions            0.3521\n",
      "  reforms              0.3467\n",
      "  methodist            0.3463\n",
      "  austrian             0.3437\n",
      "  bishop               0.3373\n",
      "  empire               0.3363\n",
      "  lutheran             0.3346\n",
      "  spanish              0.3325\n"
     ]
    }
   ],
   "source": [
    "with open('../text8_200K.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "words = preprocess_text(text)\n",
    "print(f\"Total words: {len(words):,}\")\n",
    "\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "word_to_id, id_to_word, corpus = build_vocabulary(words, vocab_size=10000)\n",
    "print(f\"Vocab: {len(word_to_id):,}, Corpus: {len(corpus):,}\")\n",
    "\n",
    "print(\"\\nBuilding co-occurrence matrix...\")\n",
    "cooccur = build_cooccurrence_matrix(corpus, len(word_to_id), window_size=5)\n",
    "\n",
    "print(\"\\nComputing PPMI...\")\n",
    "ppmi = compute_ppmi(cooccur)\n",
    "\n",
    "print(\"\\nInitializing model...\")\n",
    "model = WordVectorNN(vocab_size=len(word_to_id), embedding_dim=200)\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "train_model(model, ppmi, epochs=10, lr=0.789)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "test_words = [\"china\", \"computer\", \"phone\", \"napoleon\", \"god\", \"catholic\"]\n",
    "evaluate_model(model, test_words, word_to_id, id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:55:04.562725Z",
     "start_time": "2025-11-11T17:55:04.561526Z"
    }
   },
   "source": [
    "The goal is to turn words into meaningful numbers (embeddings) that computers can understand. the word embeddings are nothing but words converted to numbers that capture meaning of the word. the network learns by trying to predict which words appear near each other in sequences. Words that are used in similar contexts like \"Cat\" and \"dog\" end up with similar number representations. \n",
    "1. A co-occurrence matrix that counts how often words appear near each other.\n",
    "2. Then convert the raw counts to PPMI (Positive Pointwise Mutual Information), which measures if words appear together more than random chance would predict. This filters out meaningless co-occurrences.\n",
    "3. Then train the neural network to reconstruct the PPMI matrix. Input: one hot encode vector for a word -> hidden layer (embedding) -> output: predicted PPMI values for all other words. The hidden layer thus becomes the word embedding.\n",
    "4. words used in similar context like \"cat\" and \"dog\" both appear near \"pet\", \"animal\", \"furry\" will have similar PPMI patterns, so the network learns similar embeddings for them. The network is forced to compress words relationships into small embeddings to capture meaning.\n",
    "5. Thus each word gets a dense vector (100 numbers) where similar words have similar vectors.\n",
    "6. Eg: vector (\"king\") - vector (\"man\") + vector (\"woman\") = approx vector (\"queen\") -> the maths behind it. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
