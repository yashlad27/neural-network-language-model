{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 7: Word Vectors - Manual NN (20 Newsgroups)\n",
    "\n",
    "**Dataset:** 20 Newsgroups (5-8 categories)  \n",
    "**Method:** Neural network trained on PPMI matrix (Manual NumPy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:36:06.133144Z",
     "start_time": "2025-11-11T17:36:04.834235Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:36:06.143173Z",
     "start_time": "2025-11-11T17:36:06.140829Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z ]+\", ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.split()\n",
    "\n",
    "def build_vocabulary(words, vocab_size=5000, min_count=3):\n",
    "    stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'from',\n",
    "                  'is', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'this', 'that', 'it', 'as',\n",
    "                  'will', 'would', 'could', 'can', 'do', 'does', 'did', 'not', 'no', 'if', 'so'}\n",
    "    \n",
    "    word_counts = Counter(words)\n",
    "    filtered = [(w, c) for w, c in word_counts.items() \n",
    "                if c >= min_count and len(w) >= 3 and w not in stop_words]\n",
    "    \n",
    "    most_common = sorted(filtered, key=lambda x: x[1], reverse=True)[:vocab_size]\n",
    "    word_to_id = {word: idx for idx, (word, _) in enumerate(most_common)}\n",
    "    id_to_word = {idx: word for word, idx in word_to_id.items()}\n",
    "    corpus = [word_to_id[word] for word in words if word in word_to_id]\n",
    "    \n",
    "    return word_to_id, id_to_word, corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Co-occurrence Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:36:06.147526Z",
     "start_time": "2025-11-11T17:36:06.145807Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_cooccurrence_matrix(corpus, vocab_size, window_size=5):\n",
    "    cooccur = np.zeros((vocab_size, vocab_size), dtype=np.float32)\n",
    "    \n",
    "    for i in tqdm(range(len(corpus)), desc=\"Building co-occurrence\"):\n",
    "        center = corpus[i]\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(corpus), i + window_size + 1)\n",
    "        \n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                context = corpus[j]\n",
    "                distance = abs(i - j)\n",
    "                weight = 1.0 / distance\n",
    "                cooccur[center, context] += weight\n",
    "    \n",
    "    return cooccur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPMI Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:36:06.151421Z",
     "start_time": "2025-11-11T17:36:06.149613Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_ppmi(cooccur_matrix):\n",
    "    total = cooccur_matrix.sum()\n",
    "    word_counts = cooccur_matrix.sum(axis=1)\n",
    "    context_counts = cooccur_matrix.sum(axis=0)\n",
    "    \n",
    "    ppmi = np.zeros_like(cooccur_matrix)\n",
    "    \n",
    "    for i in tqdm(range(cooccur_matrix.shape[0]), desc=\"Computing PPMI\"):\n",
    "        for j in range(cooccur_matrix.shape[1]):\n",
    "            if cooccur_matrix[i, j] > 0:\n",
    "                p_ij = cooccur_matrix[i, j] / total\n",
    "                p_i = word_counts[i] / total\n",
    "                p_j = context_counts[j] / total\n",
    "                \n",
    "                pmi = np.log(p_ij / (p_i * p_j + 1e-10))\n",
    "                ppmi[i, j] = max(0, pmi)\n",
    "    \n",
    "    np.fill_diagonal(ppmi, 0)\n",
    "    row_sums = ppmi.sum(axis=1, keepdims=True)\n",
    "    ppmi_normalized = np.divide(ppmi, row_sums, where=row_sums>0)\n",
    "    \n",
    "    return ppmi_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:36:06.155744Z",
     "start_time": "2025-11-11T17:36:06.153648Z"
    }
   },
   "outputs": [],
   "source": [
    "class WordVectorNN:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.W1 = np.random.normal(0, 0.1, (vocab_size, embedding_dim)).astype(np.float32)\n",
    "        self.W2 = np.random.normal(0, 0.1, (embedding_dim, vocab_size)).astype(np.float32)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x))\n",
    "        return exp_x / (np.sum(exp_x) + 1e-10)\n",
    "    \n",
    "    def forward(self, word_idx):\n",
    "        hidden = self.W1[word_idx]\n",
    "        output = hidden @ self.W2\n",
    "        probs = self.softmax(output)\n",
    "        return hidden, probs\n",
    "    \n",
    "    def backward(self, word_idx, hidden, probs, target, lr):\n",
    "        d_output = probs - target\n",
    "        d_W2 = np.outer(hidden, d_output)\n",
    "        d_hidden = d_output @ self.W2.T\n",
    "        \n",
    "        self.W2 -= lr * d_W2\n",
    "        self.W1[word_idx] -= lr * d_hidden\n",
    "    \n",
    "    def train_step(self, word_idx, target, lr):\n",
    "        hidden, probs = self.forward(word_idx)\n",
    "        loss = -np.sum(target * np.log(probs + 1e-10))\n",
    "        self.backward(word_idx, hidden, probs, target, lr)\n",
    "        return loss\n",
    "    \n",
    "    def get_embedding(self, word_idx):\n",
    "        return self.W1[word_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:36:06.159380Z",
     "start_time": "2025-11-11T17:36:06.157861Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, ppmi_matrix, epochs=10, lr=0.789):\n",
    "    vocab_size = ppmi_matrix.shape[0]\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        indices = np.arange(vocab_size)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        for idx in tqdm(indices, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            loss = model.train_step(idx, ppmi_matrix[idx], lr)\n",
    "            total_loss += loss\n",
    "        \n",
    "        avg_loss = total_loss / vocab_size\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:36:06.163771Z",
     "start_time": "2025-11-11T17:36:06.161657Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(v1, v2):\n",
    "    norm1 = np.linalg.norm(v1)\n",
    "    norm2 = np.linalg.norm(v2)\n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0\n",
    "    return np.dot(v1, v2) / (norm1 * norm2)\n",
    "\n",
    "def find_similar_words(word, word_to_id, id_to_word, model, top_k=15):\n",
    "    if word not in word_to_id:\n",
    "        return None\n",
    "    \n",
    "    word_idx = word_to_id[word]\n",
    "    word_emb = model.get_embedding(word_idx)\n",
    "    \n",
    "    similarities = []\n",
    "    for idx in range(model.vocab_size):\n",
    "        if idx != word_idx:\n",
    "            other_emb = model.get_embedding(idx)\n",
    "            sim = cosine_similarity(word_emb, other_emb)\n",
    "            similarities.append((id_to_word[idx], sim))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_k]\n",
    "\n",
    "def evaluate_model(model, test_words, word_to_id, id_to_word):\n",
    "    for word in test_words:\n",
    "        similar = find_similar_words(word, word_to_id, id_to_word, model)\n",
    "        if similar:\n",
    "            print(f\"\\n{word.upper()}:\")\n",
    "            for w, sim in similar:\n",
    "                print(f\"  {w:20s} {sim:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:38:47.481893Z",
     "start_time": "2025-11-11T17:36:06.170139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 20 Newsgroups...\n",
      "Total words: 1,033,364\n",
      "\n",
      "Building vocabulary...\n",
      "Vocab: 10,000, Corpus: 569,458\n",
      "\n",
      "Building co-occurrence matrix...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building co-occurrence: 100%|██████████| 569458/569458 [00:01<00:00, 493245.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing PPMI...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing PPMI: 100%|██████████| 10000/10000 [00:07<00:00, 1266.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initializing model...\n",
      "\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 10000/10000 [00:14<00:00, 682.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Avg Loss: 9.2202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 10000/10000 [00:15<00:00, 649.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Avg Loss: 9.1810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 10000/10000 [00:14<00:00, 698.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Avg Loss: 9.1383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 10000/10000 [00:16<00:00, 591.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Avg Loss: 9.0817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 10000/10000 [00:17<00:00, 573.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Avg Loss: 8.9844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 10000/10000 [00:14<00:00, 674.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Avg Loss: 8.7851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 10000/10000 [00:14<00:00, 683.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Avg Loss: 8.5201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 10000/10000 [00:14<00:00, 675.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Avg Loss: 8.3245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 10000/10000 [00:14<00:00, 688.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Avg Loss: 8.1439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 10000/10000 [00:13<00:00, 729.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Avg Loss: 7.9555\n",
      "\n",
      "============================================================\n",
      "EVALUATION\n",
      "============================================================\n",
      "\n",
      "CHINA:\n",
      "  hong                 0.3731\n",
      "  kong                 0.3641\n",
      "  scheduled            0.3578\n",
      "  sponsor              0.3304\n",
      "  korean               0.3264\n",
      "  cosmos               0.3249\n",
      "  worldview            0.3243\n",
      "  ariane               0.3230\n",
      "  boycott              0.3157\n",
      "  titan                0.3146\n",
      "  kansas               0.3144\n",
      "  usaf                 0.3118\n",
      "  saturn               0.3102\n",
      "  chinese              0.3088\n",
      "  distributor          0.3079\n",
      "\n",
      "COMPUTER:\n",
      "  applications         0.3993\n",
      "  ncsa                 0.3953\n",
      "  advanced             0.3848\n",
      "  verlag               0.3766\n",
      "  ieee                 0.3740\n",
      "  tel                  0.3684\n",
      "  pub                  0.3635\n",
      "  corp                 0.3608\n",
      "  interactive          0.3588\n",
      "  xerox                0.3564\n",
      "  geometry             0.3561\n",
      "  convex               0.3536\n",
      "  systems              0.3529\n",
      "  laboratory           0.3523\n",
      "  idl                  0.3490\n",
      "\n",
      "PHONE:\n",
      "  cost                 0.3811\n",
      "  canberra             0.3731\n",
      "  cud                  0.3462\n",
      "  communication        0.3321\n",
      "  bis                  0.3267\n",
      "  jeffrey              0.3265\n",
      "  aips                 0.3205\n",
      "  hints                0.3198\n",
      "  contact              0.3174\n",
      "  proposals            0.3127\n",
      "  gibbons              0.3106\n",
      "  algebra              0.3103\n",
      "  fax                  0.3102\n",
      "  nren                 0.3097\n",
      "  txt                  0.3095\n",
      "\n",
      "NAPOLEON:\n",
      "  responds             0.3948\n",
      "  chris                0.3947\n",
      "  andy                 0.3650\n",
      "  pride                0.3627\n",
      "  manuscript           0.3528\n",
      "  repent               0.3406\n",
      "  concentrate          0.3386\n",
      "  idiots               0.3384\n",
      "  lights               0.3365\n",
      "  dare                 0.3340\n",
      "  quran                0.3320\n",
      "  confusing            0.3290\n",
      "  figured              0.3279\n",
      "  josh                 0.3275\n",
      "  thessaloniki         0.3267\n",
      "\n",
      "GOD:\n",
      "  sins                 0.3579\n",
      "  sexual               0.3447\n",
      "  belief               0.3363\n",
      "  refuse               0.3252\n",
      "  gift                 0.3216\n",
      "  lord                 0.3155\n",
      "  blood                0.3076\n",
      "  withdraw             0.3052\n",
      "  love                 0.3029\n",
      "  faith                0.2986\n",
      "  atheists             0.2978\n",
      "  preaching            0.2973\n",
      "  fathers              0.2947\n",
      "  fulfill              0.2893\n",
      "  immaculate           0.2882\n",
      "\n",
      "CATHOLIC:\n",
      "  churches             0.4321\n",
      "  trinity              0.4067\n",
      "  referring            0.3990\n",
      "  lutheran             0.3914\n",
      "  scripture            0.3797\n",
      "  pope                 0.3797\n",
      "  realm                0.3791\n",
      "  saint                0.3746\n",
      "  affirmed             0.3736\n",
      "  savior               0.3724\n",
      "  messiah              0.3716\n",
      "  practices            0.3705\n",
      "  answered             0.3705\n",
      "  church               0.3676\n",
      "  doctrine             0.3654\n"
     ]
    }
   ],
   "source": [
    "categories = [\n",
    "    'comp.graphics', 'comp.sys.ibm.pc.hardware',\n",
    "    'sci.crypt', 'sci.space',\n",
    "    'soc.religion.christian',\n",
    "    'talk.politics.guns', 'talk.politics.mideast',\n",
    "    'alt.atheism'\n",
    "]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "text = ' '.join(newsgroups.data)\n",
    "words = preprocess_text(text)\n",
    "print(f\"Total words: {len(words):,}\")\n",
    "\n",
    "print(\"\\nBuilding vocabulary...\")\n",
    "word_to_id, id_to_word, corpus = build_vocabulary(words, vocab_size=10000)\n",
    "print(f\"Vocab: {len(word_to_id):,}, Corpus: {len(corpus):,}\")\n",
    "\n",
    "print(\"\\nBuilding co-occurrence matrix...\")\n",
    "cooccur = build_cooccurrence_matrix(corpus, len(word_to_id), window_size=5)\n",
    "\n",
    "print(\"\\nComputing PPMI...\")\n",
    "ppmi = compute_ppmi(cooccur)\n",
    "\n",
    "print(\"\\nInitializing model...\")\n",
    "model = WordVectorNN(vocab_size=len(word_to_id), embedding_dim=200)\n",
    "\n",
    "print(\"\\nTraining...\")\n",
    "train_model(model, ppmi, epochs=10, lr=0.789)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "test_words = [\"china\", \"computer\", \"phone\", \"napoleon\", \"god\", \"catholic\"]\n",
    "evaluate_model(model, test_words, word_to_id, id_to_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T17:38:48.363575Z",
     "start_time": "2025-11-11T17:38:48.362562Z"
    }
   },
   "source": [
    "The same idea as Text8 but on newsgroup discussions. The network learns word meaning from how people use them in different discussion topics like tech, politics, sports. By training on multiple categories, the model learns that words like \"ball\" mean different things in sports vs political contexts. This helps nn know why context matters and the network figures out word relationships by seeing how they are actually used in real conversations. \n",
    "\n",
    "By training on 8 different newsgroup categories, the model sees words used in various contexts. \"windows\" appears near \"microsoft\", \"software\" in tech groups but \"windows\" might appear near  \"house\", \"glass\", \"frame\" in home groups, so context matters and model learning both meanings from context.\n",
    "\n",
    "similar algorithm:\n",
    "1. PPMI\n",
    "2. co-occurrence matrix with window_size 5\n",
    "3. inverse distace weighting\n",
    "4. Neural network: vocab_size * embedding_dim * vocab_size\n",
    "5. manual backpropagation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
